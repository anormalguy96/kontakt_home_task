{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NER Extractor (Synthetic BIO)\n",
        "\n",
        "Bu notebook 2-ci mərhələni (Extracting) göstərir.\n",
        "\n",
        "Guardrail (classifier) bir mesajı UNSAFE kimi işarələyirsə, burada işə düşən NER modeli mesajın içindəki PII hissələrini tapır və sonradan həmin hissələri `****` ilə maskalamağa imkan verir.\n",
        "\n",
        "#### Niyə sintetik data?\n",
        "Çünki FİN və telefon formatları bizdə spesifikdir və real NER data toplamaq çətin ola bilər. Ona görə əvvəlcə sintetik BIO dataset ilə pipeline-i oturduruq."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os, sys, subprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64901eda",
      "metadata": {},
      "source": [
        "Notebook-lar `task3/notebooks/` içində olacaq. Repo root-a çıxırıq."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "becc47f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo root: d:\\github_repos\\kontakt_home_task\\task3\n"
          ]
        }
      ],
      "source": [
        "ROOT = Path.cwd()\n",
        "if ROOT.name.lower() == \"notebooks\":\n",
        "    ROOT = ROOT.parent\n",
        "\n",
        "os.chdir(ROOT)\n",
        "print(\"Repo root:\", Path.cwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecea39de",
      "metadata": {},
      "source": [
        "Script və modul run-ları üçün ən stabil variant: PYTHONPATH-a src əlavə etməkdir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e12a786e",
      "metadata": {},
      "outputs": [],
      "source": [
        "PYTHONPATH = str(Path.cwd() / \"src\")\n",
        "ENV = os.environ.copy()\n",
        "ENV[\"PYTHONPATH\"] = PYTHONPATH + (os.pathsep + ENV[\"PYTHONPATH\"] if ENV.get(\"PYTHONPATH\") else \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c032d204",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run(cmd):\n",
        "    \"\"\"Always run with the same python as this notebook kernel + correct PYTHONPATH.\"\"\"\n",
        "    if isinstance(cmd, str):\n",
        "        cmd = cmd.split()\n",
        "    print(\"RUN:\", \" \".join(cmd))\n",
        "    return subprocess.run(cmd, env=ENV, check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Seqeval dependency"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b0b115d",
      "metadata": {},
      "source": [
        "### Niyə `seqeval` lazımdır?\n",
        "\n",
        "NER üçün “accuracy” təkbaşına çox şey demir. Əsas metriklər:\n",
        "- precision\n",
        "- recall\n",
        "- F1 Score-dur\n",
        "\n",
        "`evaluate.load(\"seqeval\")` da məhz bu hesablamanı edir. `seqeval` qurulmasa, training zamanı metrik hissəsi error verəcək."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RUN: d:\\github_repos\\kontakt_home_task\\.venv\\Scripts\\python.exe -m pip install -q seqeval\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CompletedProcess(args=['d:\\\\github_repos\\\\kontakt_home_task\\\\.venv\\\\Scripts\\\\python.exe', '-m', 'pip', 'install', '-q', 'seqeval'], returncode=0)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"seqeval\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Generate synthetic data (5k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RUN: d:\\github_repos\\kontakt_home_task\\.venv\\Scripts\\python.exe scripts/synthetic_ner_generator.py --n 5000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CompletedProcess(args=['d:\\\\github_repos\\\\kontakt_home_task\\\\.venv\\\\Scripts\\\\python.exe', 'scripts/synthetic_ner_generator.py', '--n', '5000'], returncode=0)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run([sys.executable, \"scripts/synthetic_ner_generator.py\", \"--n\", \"5000\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Train NER (1 epoch, batch 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RUN: d:\\github_repos\\kontakt_home_task\\.venv\\Scripts\\python.exe -m pii_guard.training.train_ner --epochs 1 --batch 128 --max_len 24\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CompletedProcess(args=['d:\\\\github_repos\\\\kontakt_home_task\\\\.venv\\\\Scripts\\\\python.exe', '-m', 'pii_guard.training.train_ner', '--epochs', '1', '--batch', '128', '--max_len', '24'], returncode=0)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run([sys.executable, \"-m\", \"pii_guard.training.train_ner\", \"--epochs\", \"1\", \"--batch\", \"128\", \"--max_len\", \"24\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad0825f",
      "metadata": {},
      "source": [
        "## Training parametrləri niyə belədir?\n",
        "\n",
        "Mənim mühit CPU-ludur, yəni GPU yoxdur.\n",
        "Ona görə “production-grade training” yox, sürətli iterasiya seçmişəm.\n",
        "Yəni\n",
        "- `epochs=1` → sadəcə pipeline-i işlətmək üçün\n",
        "- `batch=128` → step sayı azalsın deyə\n",
        "- `max_len=24` → ən böyük sürət qazancı buradan gəlir (time complexity `O(L^2)` olduğu üçün)\n",
        "\n",
        "Qeyd:\n",
        "* Əgər `train_ner.py`-də `--max_len` arqumenti əlavə etməmisinizsə, bu komanda error verər.\n",
        "* O halda ya `--max_len`-i silin, ya da `train_ner.py`-də tokenizer çağırışına `max_length=args.max_len` əlavə edin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Minimal masking demo (PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ebd8fed5",
      "metadata": {},
      "outputs": [],
      "source": [
        "LABELS = [\n",
        "    \"O\",\n",
        "    \"B-FIN\", \"I-FIN\",\n",
        "    \"B-CARD\", \"I-CARD\",\n",
        "    \"B-PHONE\", \"I-PHONE\",\n",
        "    \"B-PERSON\", \"I-PERSON\",\n",
        "]\n",
        "ID2LABEL = {i: l for i, l in enumerate(LABELS)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "938cf526",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer you are loading from 'models/ner/pytorch' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DistilBertForTokenClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tok = AutoTokenizer.from_pretrained(\"models/ner/pytorch\", use_fast=True)\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"models/ner/pytorch\")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6035bce",
      "metadata": {},
      "source": [
        "## Demo hissəsi nəyi göstərir?\n",
        "\n",
        "Burada real sistem kimi “maskalama”nı göstəririk:\n",
        "\n",
        "1) mətn tokenləşir (offset-lər götürülür)\n",
        "2) model hər token üçün label proqnozlaşdırır\n",
        "3) BIO label-lardan spanlar yığılır (məs: PHONE, FIN və s.)\n",
        "4) həmin spanlar `****` ilə əvəzlənir\n",
        "\n",
        "Bu demo sadəcə “gözlə görülən nəticə” üçündür. Təbii ki production-da əlavə qaydalar (edge-case-lər, overlap, false positive-lər) tələb olunur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "68c2699c",
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"Mənim adım Elvin Əliyevdir, FİN 94FMDDD və telefon +994 50 123 45 67.\"\n",
        "enc = tok(text, return_tensors=\"pt\", truncation=True, max_length=96, return_offsets_mapping=True)\n",
        "offsets = enc.pop(\"offset_mapping\")[0].tolist()\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**enc).logits[0].cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "558781e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_ids = logits.argmax(axis=-1).tolist()\n",
        "\n",
        "spans = []\n",
        "current = None\n",
        "for pid, (s, e) in zip(pred_ids, offsets):\n",
        "    if s == 0 and e == 0:\n",
        "        continue\n",
        "    lab = ID2LABEL.get(pid, \"O\")\n",
        "    if lab == \"O\":\n",
        "        if current:\n",
        "            spans.append(current); current = None\n",
        "        continue\n",
        "    if lab.startswith(\"B-\"):\n",
        "        if current:\n",
        "            spans.append(current)\n",
        "        current = (lab[2:], s, e)\n",
        "    else:  # I-\n",
        "        ent = lab[2:]\n",
        "        if current and current[0] == ent:\n",
        "            current = (current[0], current[1], e)\n",
        "        else:\n",
        "            if current:\n",
        "                spans.append(current)\n",
        "            current = (ent, s, e)\n",
        "\n",
        "if current:\n",
        "    spans.append(current)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e1ddd27c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEXT  : Mənim adım Elvin Əliyevdir, FİN 94FMDDD və telefon +994 50 123 45 67.\n",
            "SPANS : []\n",
            "MASKED: Mənim adım Elvin Əliyevdir, FİN 94FMDDD və telefon +994 50 123 45 67.\n"
          ]
        }
      ],
      "source": [
        "spans.sort(key=lambda x: x[1])\n",
        "\n",
        "masked = []\n",
        "last = 0\n",
        "for ent, s, e in spans:\n",
        "    masked.append(text[last:s])\n",
        "    masked.append(\"****\")\n",
        "    last = e\n",
        "masked.append(text[last:])\n",
        "\n",
        "print(\"TEXT  :\", text)\n",
        "print(\"SPANS :\", spans)\n",
        "print(\"MASKED:\", \"\".join(masked))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad052465",
      "metadata": {},
      "source": [
        "### Qısa qeydlər (real life olsaydı nələr dəyişə bilərdi?)\n",
        "\n",
        "- Token-based NER bəzən sərhədləri tam tutmaya bilər (xüsusilə adlar və qarışıq cümlələr)\n",
        "- Ona görə praktikada çox vaxt **NER + regex** birlikdə işlədilir:\n",
        "  - regex: FIN / PHONE / CARD kimi “formatı dəqiq” şeylər\n",
        "  - NER: PERSON kimi daha “yumşaq” entity-lər\n",
        "\n",
        "Bu taskda da məqsəd cascading arxitekturanın işlədiyini və optimizasiyaya hazır olduğunu göstərməkdir.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
