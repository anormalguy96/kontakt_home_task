{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ONNX export + INT8 quantization + Benchmark\n",
        "\n",
        "Bu notebook layihənin “sürət və optimizasiya” hissəsini bağlayır.\n",
        "\n",
        "Məqsəd:\n",
        "- PyTorch modelləri ONNX formatına çevirmək (CPU inference üçün daha sürətli)\n",
        "- ONNX modelləri INT8 (dynamic quantization) ilə sıxmaq\n",
        "- Ölçü və sürət müqayisəsi aparmaq (PyTorch vs ONNX vs ONNX INT8)\n",
        "\n",
        "Bu hissə çox vacibdir, çünki təkcə \"model varmı?\" sualına yox, \"model CPU-da necə sürətli işləyir?\" göstərir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "79ad1fb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os, sys, subprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84fd32dc",
      "metadata": {},
      "source": [
        "Az əvvəlki Python skriptini yenə run edirik:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo root: d:\\github_repos\\kontakt_home_task\\task3\n"
          ]
        }
      ],
      "source": [
        "ROOT = Path.cwd()\n",
        "if ROOT.name.lower() == \"notebooks\":\n",
        "    ROOT = ROOT.parent\n",
        "\n",
        "os.chdir(ROOT)\n",
        "print(\"Repo root:\", Path.cwd())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c90285ca",
      "metadata": {},
      "source": [
        "Script və modul run-ları üçün ən stabil variant `PYTHONPATH`a **src** əlavə etmək"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "65159c99",
      "metadata": {},
      "outputs": [],
      "source": [
        "PYTHONPATH = str(Path.cwd() / \"src\")\n",
        "ENV = os.environ.copy()\n",
        "ENV[\"PYTHONPATH\"] = PYTHONPATH + (os.pathsep + ENV[\"PYTHONPATH\"] if ENV.get(\"PYTHONPATH\") else \"\")\n",
        "\n",
        "def run(cmd):\n",
        "    \"\"\"Always run with the same python as this notebook kernel + correct PYTHONPATH.\"\"\"\n",
        "    if isinstance(cmd, str):\n",
        "        cmd = cmd.split()\n",
        "    print(\"RUN:\", \" \".join(cmd))\n",
        "    return subprocess.run(cmd, env=ENV, check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e1288ee",
      "metadata": {},
      "source": [
        "### Bu notebook-da niyə `run()` istifadə edirəm?\n",
        "\n",
        "Eyni səbəb: notebook hansı Python interpreter ilə açılıbsa, bütün komandalar da həmin interpreter ilə işləsin.\n",
        "\n",
        "Əks halda:\n",
        "- `python` PATH-dan başqa yerə düşə bilər\n",
        "- `pii_guard` import problemi çıxa bilər\n",
        "- ONNX faylları başqa environmentdə yazılıb burada tapılmaya bilər"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Export ONNX\n",
        "\n",
        "Bu nə üçündür?\n",
        "\n",
        "PyTorch modeli birbaşa CPU-da işləyə bilər, amma ONNX Runtime çox vaxt daha sürətli olur.\n",
        "\n",
        "Export prosesində:\n",
        "- `models/classifier/pytorch`  →  `models/classifier/onnx`\n",
        "- `models/ner/pytorch`         →  `models/ner/onnx`\n",
        "\n",
        "Nəticədə `model.onnx` və tokenizer/config faylları həmin qovluqlarda olur.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RUN: d:\\github_repos\\kontakt_home_task\\.venv\\Scripts\\python.exe -m pii_guard.optimization.export_onnx\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CompletedProcess(args=['d:\\\\github_repos\\\\kontakt_home_task\\\\.venv\\\\Scripts\\\\python.exe', '-m', 'pii_guard.optimization.export_onnx'], returncode=0)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run([sys.executable, \"-m\", \"pii_guard.optimization.export_onnx\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Quantize INT8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RUN: d:\\github_repos\\kontakt_home_task\\.venv\\Scripts\\python.exe -m pii_guard.optimization.quantize\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CompletedProcess(args=['d:\\\\github_repos\\\\kontakt_home_task\\\\.venv\\\\Scripts\\\\python.exe', '-m', 'pii_guard.optimization.quantize'], returncode=0)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run([sys.executable, \"-m\", \"pii_guard.optimization.quantize\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1614106",
      "metadata": {},
      "source": [
        "### Bəs INT8 Quantization nə üçündür?\n",
        "\n",
        "INT8 quantization modelin çəkilərini 8-bit formatına salır.\n",
        "\n",
        "Praktik effekt budur ki,\n",
        "- model ölçüsü ciddi kiçilir (məs: 500MB → 120-130MB)\n",
        "- inference CPU-da daha sürətli olur\n",
        "\n",
        "Burada dynamic quantization istifadə olunur.\n",
        "- Bu sayədə həm ataset calibration tələb etmir\n",
        "- həm də pipeline üçün rahatdır"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Benchmark (n=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RUN: d:\\github_repos\\kontakt_home_task\\.venv\\Scripts\\python.exe -m pii_guard.optimization.benchmark --n 80\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CompletedProcess(args=['d:\\\\github_repos\\\\kontakt_home_task\\\\.venv\\\\Scripts\\\\python.exe', '-m', 'pii_guard.optimization.benchmark', '--n', '80'], returncode=0)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run([sys.executable, \"-m\", \"pii_guard.optimization.benchmark\", \"--n\", \"80\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Report-u cədvəl kimi göstərək"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4f39c18c",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import pandas as pd\n",
        "except Exception:\n",
        "    run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pandas\"])\n",
        "    import pandas as pd\n",
        "\n",
        "bench = json.loads(Path(\"reports/benchmarks/bench.json\").read_text(encoding=\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "95bef5e0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>size_mb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ner_onnx_int8</th>\n",
              "      <td>128.870521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classifier_onnx_int8</th>\n",
              "      <td>129.432046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ner_onnx</th>\n",
              "      <td>514.102571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classifier_onnx</th>\n",
              "      <td>516.335464</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         size_mb\n",
              "ner_onnx_int8         128.870521\n",
              "classifier_onnx_int8  129.432046\n",
              "ner_onnx              514.102571\n",
              "classifier_onnx       516.335464"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>latency_ms_avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>classifier_onnx_int8_safe</th>\n",
              "      <td>12.456330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ner_onnx_int8</th>\n",
              "      <td>19.149159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ner_onnx</th>\n",
              "      <td>43.471030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classifier_onnx_safe</th>\n",
              "      <td>50.442089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classifier_pytorch_unsafe</th>\n",
              "      <td>109.753114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ner_pytorch</th>\n",
              "      <td>124.147169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classifier_pytorch_safe</th>\n",
              "      <td>149.382431</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           latency_ms_avg\n",
              "classifier_onnx_int8_safe       12.456330\n",
              "ner_onnx_int8                   19.149159\n",
              "ner_onnx                        43.471030\n",
              "classifier_onnx_safe            50.442089\n",
              "classifier_pytorch_unsafe      109.753114\n",
              "ner_pytorch                    124.147169\n",
              "classifier_pytorch_safe        149.382431"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sizes = pd.Series(bench[\"sizes_mb\"], name=\"size_mb\").to_frame().sort_values(\"size_mb\")\n",
        "lat = pd.Series(bench[\"latency_ms_avg\"], name=\"latency_ms_avg\").to_frame().sort_values(\"latency_ms_avg\")\n",
        "\n",
        "display(sizes)\n",
        "display(lat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aecfa83f",
      "metadata": {},
      "source": [
        "Bu hissədə JSON-u oxuyub daha rahat görmək üçün cədvəl kimi göstərirəm.\n",
        "\n",
        "Məqsəd həm hansı variantın daha sürətli olduğunu, həm də ölçünün necə dəyişdiyini konkret rəqəmlərlə göstərməkdir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a213c0c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Guardrail speedup: ONNX=2.96x, INT8=11.99x\n"
          ]
        }
      ],
      "source": [
        "pt = bench[\"latency_ms_avg\"][\"classifier_pytorch_safe\"]\n",
        "onnx = bench[\"latency_ms_avg\"][\"classifier_onnx_safe\"]\n",
        "int8 = bench[\"latency_ms_avg\"][\"classifier_onnx_int8_safe\"]\n",
        "print(f\"Guardrail speedup: ONNX={pt/onnx:.2f}x, INT8={pt/int8:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7610c8df",
      "metadata": {},
      "source": [
        "## Qısa yekun (real sistemdə nə alınır?)\n",
        "\n",
        "Bu notebook-un nəticəsi adətən (*adətən* dedim, çünki GPU-nun olub-olmaması nəticəni qismən dəyişə bilər) belə olur:\n",
        "\n",
        "- ONNX, PyTorch-dan xeyli sürətli çıxır\n",
        "- INT8 ONNX ən sürətli variant olur\n",
        "- Model ölçüsü də ciddi azalır\n",
        "\n",
        "Bu da Guardrail üçün 20ms-dən aşağı” latency hədəfinə yaxınlaşmağın ən real yoludur (CPU mühitində :D).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc725356",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
